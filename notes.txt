
6th December

1. 2 GPU - [632.82, 266.64], [629.93, 265.21], [630.3, 271.52], [630.72, 273.38], [630.02, 269.57]
2. 3 GPU - [409.46, 269.43], [410.85, 275.21], [410.79, 264.86], [410.74, 277.39], [409.55, 275.30]  
3. 4 GPU - [290.86, 273.07], [290.91, 279.45], [291.11, 271.14], [291.24, 272.44], [290.99, 274.62] 


Plot the learning curves (i.e. loss as training progresses)

1. 2 GPU, 32 batch size on each - each tuple is (time, loss) after a fixed number of steps. loss is averaged in case of async training. 

Synch: [99.89, 270.2], [185.28, 269.89], [271.04, 267.51], [357.07, 266.33], [443.08, 250.47], [529.06, 246.65], [615.09, 255.17] 
Asynch: [73.23, 289.19], [142.05, 270.27], [211, 276.61], [280.04, 266.51], [348.92, 271.37], [417.80, 272.76], [486.73, 272.03], [555.31, 268.28], [624.31, 271.9]
 
2. 3 GPU, 32 batch size on each 

Synch: [75.57, 267.71], [138.42, 260.93], [200.26, 264.78], [262.11, 258.85], [323.80, 255.77], [385.56, 259.03], [447.43, 251.95], [508.94, 251.76], [596.98, 249.84]
Asynch: [50.76, 288.56], [98.31, 277.06], [145.65, 278.63], [193.07, 270], [240.41, 275.27], [287.71, 271.75], [335.04, 272.1], [382.19, 271.41]


2nd Decemeber

1. Link for how synchronous training works - https://pytorch.org/docs/stable/notes/ddp.html
2. TODO: Figure out why eval at end of training is incorrec for asynchronous training

Results on the 'sentiment' section of the tweeteval dataset:

Baseline:
a. 1 GPU, 1 proc, 32 batch size: Eval loss = 498.35, Time = 2278.50 sec

Asynchronous:
a. 2 GPU, 2 proc, 32 batch size each: Eval loss = 529.83, 539.78, Time = 1139.34 sec
b. 3 GPU, 3 proc, 32 batch size each: Eval loss = 536,75, 526.14, 522.83, Time = 804.73 sec

Synchronous:
a. 2 GPU, 32 batch size each: Eval loss = 490.39, 494.53, Time = 1518.54 sec 
b. 3 GPU, 32 batch size each: Eval loss = 511.64, 500.06, 478.93, Time = 1099.09 sec

1st December

Some intial results:

1. MLM training on hate-speech detection: 

Asynchornous : 
a. 1 GPU, 2 proc, 32 batch size each: Eval loss = 356.05, Time = 1442.69 sec
b. 1 GPU, 4 proc, 16 batch size each: Eval loss = 364.44, Time = 1530.95 sec
c. 2 GPU, 2 proc, 32 batch size each: Eval loss = 252.46, 256.68, Time = 670.78 sec
d. 3 GPU, 3 proc, 32 batch size each: Eval loss = 263.60, 274.92, 265.68, Time = 448.63 sec
e. 4 GPU, 4 proc, 32 batch size each: Eval loss = 271.13, 276.38, 282.15, 277.05,Time = 340.29 sec 

Synchronous (not really, it's just one process) : 
a. 1 GPU, 32 batch size: Eval loss = 274.37, Time = 1294.008 sec
b. 1 GPU, 64 batch size: Eval loss = 258.66, Time = 1259.63 sec
c. 2 GPU, 64 total batch size: Eval loss = 248.02, Time = 1126.58 sec

Synchronous :
a. 2 GPU, 64 total batch size: Eval loss: 239.79, 256.04, Time = 819.22 sec
b. 3 GPU, 96 total batch size: Eval loss: 248.08, 246.70, 251.14, TIme = 581.81 sec 
c. 4 GPU, 32*4 total batch size: Eval loss: 254.05, 254.65, 263.18, 248.94, Time = 453.15 sec 


17th Nov

How can we simulate the pretrainig without requiring exorbitant amount of compute?
--- There are two options i think:
1. Pretrain transformers using MLM on very small dataset. The advantage of this is that we'll be able to fully isolate the effects of synchronous vs asynchornous SGD since they are used to train a model from scratch. The downside of this approach is that if the model is pretrained on a very small dataset, the effect of pretraining on the downstream task will be minimal, maybe giving unnoticalable differences between the two distributed optimization methods.
2. Use STILT (i.e. continue pretraining) as a proxy for pretrainig. Again we can use small datasets here which is good for faster experiments. Another advantage is that the net effect of pretraining will still be large on the downstream task, so maybe we can see more clear differences. But one important caveat here is that the two different distributed optimization algorithms will only be applied for a very small fraction of the pretraining, which again risks giving similar models.

Considering all the pros and cons, I think it is better to go with option 2. Firstly, that has been reliably used in the literature before. Second the fine-tuned models with option 2 will be much better performing & useful i.e. closer to actual SOTA. Therefore whatever analysis I do in this project will at least be somewhat useful practically. The first option beards the risk of the analysis being completely useless.
